{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":442057,"sourceType":"datasetVersion","datasetId":161598},{"sourceId":148667129,"sourceType":"kernelVersion"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Road segmentation for ADAS- CPU","metadata":{"papermill":{"duration":0.005359,"end_time":"2023-10-26T17:48:50.287887","exception":false,"start_time":"2023-10-26T17:48:50.282528","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn import utils as sk_utils\nimport os\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport cv2\n\nfrom IPython.display import clear_output\nclear_output()\nprint(tf.__version__)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-26T17:48:50.299665Z","iopub.status.busy":"2023-10-26T17:48:50.2993Z","iopub.status.idle":"2023-10-26T17:49:03.671642Z","shell.execute_reply":"2023-10-26T17:49:03.669898Z"},"papermill":{"duration":13.382875,"end_time":"2023-10-26T17:49:03.675709","exception":false,"start_time":"2023-10-26T17:48:50.292834","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.004992,"end_time":"2023-10-26T17:49:03.686174","exception":false,"start_time":"2023-10-26T17:49:03.681182","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{"papermill":{"duration":0.005062,"end_time":"2023-10-26T17:49:03.696701","exception":false,"start_time":"2023-10-26T17:49:03.691639","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Directory containing the images\ntrain_images_path = '/kaggle/input/solesensei_bdd100k/bdd100k_seg/bdd100k/seg/images/train/*.jpg'\ntrain_masks_path = '/kaggle/input/solesensei_bdd100k/bdd100k_seg/bdd100k/seg/labels/train/*.png'\nval_images_path = '/kaggle/input/solesensei_bdd100k/bdd100k_seg/bdd100k/seg/images/val/*.jpg'\nval_masks_path = '/kaggle/input/solesensei_bdd100k/bdd100k_seg/bdd100k/seg/labels/val/*.png'","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:49:03.709436Z","iopub.status.busy":"2023-10-26T17:49:03.708747Z","iopub.status.idle":"2023-10-26T17:49:03.717012Z","shell.execute_reply":"2023-10-26T17:49:03.714013Z"},"papermill":{"duration":0.018067,"end_time":"2023-10-26T17:49:03.720014","exception":false,"start_time":"2023-10-26T17:49:03.701947","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimage_size = (192, 256)\n\n# Load and preprocess the images\ndef load_image(file_path, is_x=False):\n    # Read the image file using tf.io.read_file\n    image = tf.io.read_file(file_path)\n    # Decode the image into a tensor\n    image = tf.image.decode_image(image)\n    \n    if is_x:\n        # Resize the image to the desired size using Lanczos3 method\n        image = tf.image.resize(image, image_size, method=tf.image.ResizeMethod.LANCZOS3)\n        image = image / 255.0  # Normalize the image\n    else:\n        # Resize the image to the desired size using 'nearest' method\n        image = tf.image.resize(image, image_size, method='nearest', antialias=True)\n    return image","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:49:03.732503Z","iopub.status.busy":"2023-10-26T17:49:03.732027Z","iopub.status.idle":"2023-10-26T17:49:03.740564Z","shell.execute_reply":"2023-10-26T17:49:03.73818Z"},"papermill":{"duration":0.018417,"end_time":"2023-10-26T17:49:03.743814","exception":false,"start_time":"2023-10-26T17:49:03.725397","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the training set\n\n# Get the list of image file paths\nimage_paths = sorted(glob.glob(train_images_path))\nmask_paths = sorted(glob.glob(train_masks_path))\n\n# Shuffle the image_paths and mask_paths\nimage_paths, mask_paths = sk_utils.shuffle(image_paths, mask_paths, random_state=42)\n\n# Create lists to store the loaded and preprocessed images\nX_train = []\nY_train = []\n\n# Apply the load_image function to each image path in the dataset\nfor x, y in zip(image_paths, mask_paths):\n    X_train.append(load_image(x, True))\n    Y_train.append(load_image(y))\n    if len(X_train) % 500 == 0:\n        print(len(X_train), 'data points loaded!')\nelse:\n    print(len(X_train), 'data points loaded in total!')\n\n# Convert the lists to NumPy arrays\nX_train = np.array(X_train)\nY_train = np.array(Y_train)\n# 255 is representing unknown objects\nY_train[Y_train == 255] = 19","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:49:03.757029Z","iopub.status.busy":"2023-10-26T17:49:03.756478Z","iopub.status.idle":"2023-10-26T17:52:18.322109Z","shell.execute_reply":"2023-10-26T17:52:18.320486Z"},"papermill":{"duration":194.575296,"end_time":"2023-10-26T17:52:18.324539","exception":false,"start_time":"2023-10-26T17:49:03.749243","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the validataion set (1000 observations)\n\n# Get the list of image file paths\nimage_paths = sorted(glob.glob(val_images_path))\nmask_paths = sorted(glob.glob(val_masks_path))\n\n# Shuffle the image_paths and mask_paths\nimage_paths, mask_paths = sk_utils.shuffle(image_paths, mask_paths, random_state=42)\n\n# Create lists to store the loaded and preprocessed images\nX_val = []\nY_val = []\n\n# Apply the load_image function to each image path in the dataset\nfor x, y in zip(image_paths, mask_paths):\n    X_val.append(load_image(x, True))\n    Y_val.append(load_image(y))\n    if len(X_val) == 500:\n        print(len(X_val), 'data points loaded!')\nelse:\n    print(len(X_val), 'data points loaded in total!')\n\n# Convert the lists to NumPy arrays\nX_val = np.array(X_val)\nY_val = np.array(Y_val)\n# 255 is representing unknown objects\nY_val[Y_val == 255] = 19","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:52:18.338952Z","iopub.status.busy":"2023-10-26T17:52:18.338591Z","iopub.status.idle":"2023-10-26T17:52:45.008465Z","shell.execute_reply":"2023-10-26T17:52:45.007131Z"},"papermill":{"duration":26.679009,"end_time":"2023-10-26T17:52:45.010245","exception":false,"start_time":"2023-10-26T17:52:18.331236","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('X train:', X_train.shape)\nprint('Y train:', Y_train.shape)\nprint('X val:', X_val.shape)\nprint('Y val:', Y_val.shape)","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:52:45.023794Z","iopub.status.busy":"2023-10-26T17:52:45.023444Z","iopub.status.idle":"2023-10-26T17:52:45.030291Z","shell.execute_reply":"2023-10-26T17:52:45.02885Z"},"papermill":{"duration":0.017138,"end_time":"2023-10-26T17:52:45.033341","exception":false,"start_time":"2023-10-26T17:52:45.016203","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Utils\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ncolor_dict = {\n    0: (0.7, 0.7, 0.7),     # road - gray\n    1:  (0.9, 0.9, 0.2),     # sidewalk - light yellow\n    2: (1.0, 0.4980392156862745, 0.054901960784313725),\n    3: (1.0, 0.7333333333333333, 0.47058823529411764),\n    4: (0.8, 0.5, 0.1),  # Fence - rust orange\n    5: (0.596078431372549, 0.8745098039215686, 0.5411764705882353),\n    6: (0.325, 0.196, 0.361),\n    7: (1.0, 0.596078431372549, 0.5882352941176471),\n    8:  (0.2, 0.6, 0.2),     # vegetation - green\n    9: (0.7725490196078432, 0.6901960784313725, 0.8352941176470589),\n    10: (0.5, 0.7, 1.0),     # sky - light blue\n    11: (1.0, 0.0, 0.0), # person - red\n    12: (0.8901960784313725, 0.4666666666666667, 0.7607843137254902),\n    13: (0.0, 0.0, 1.0),  # Car - blue\n    14: (0.0, 0.0, 1.0),  # Track - blue\n    15: (0.0, 0.0, 1.0),  # Bus - blue\n    16: (0.7372549019607844, 0.7411764705882353, 0.13333333333333333),\n    17: (0.8588235294117647, 0.8588235294117647, 0.5529411764705883),\n    18: (0.09019607843137255, 0.7450980392156863, 0.8117647058823529),\n    19: (0, 0, 0) # unknown - black\n}\n\ndef colorize_image(image, color_dict):\n    # remove the extra dimension\n    image = np.squeeze(image)\n    # Generate the colored image using the color dictionary\n    colored_image = np.zeros((image.shape[0], image.shape[1], 3))\n\n    for pixel_value, color in color_dict.items():\n        colored_image[image == pixel_value] = color\n\n    # Convert the image to 8-bit unsigned integer\n    colored_image = (colored_image * 255).astype(np.uint8)\n\n    return colored_image\n","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:52:45.049703Z","iopub.status.busy":"2023-10-26T17:52:45.048294Z","iopub.status.idle":"2023-10-26T17:52:45.0589Z","shell.execute_reply":"2023-10-26T17:52:45.057717Z"},"papermill":{"duration":0.02132,"end_time":"2023-10-26T17:52:45.061416","exception":false,"start_time":"2023-10-26T17:52:45.040096","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize some samples from the training set\nplt.figure(figsize=(10, 40))\ns, e = 80, 84\nindex = 1\n\nfor i, j in zip(X_train[s:e], Y_train[s:e]):\n    plt.subplot(10, 2, index)\n    plt.imshow(i)\n    plt.title('Ground truth image')\n    \n    plt.subplot(10, 2, index+1)\n    plt.imshow(colorize_image(j, color_dict))\n    plt.title('Ground truth mask')\n    index += 2","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:52:45.075681Z","iopub.status.busy":"2023-10-26T17:52:45.075231Z","iopub.status.idle":"2023-10-26T17:52:46.98853Z","shell.execute_reply":"2023-10-26T17:52:46.9869Z"},"papermill":{"duration":1.925416,"end_time":"2023-10-26T17:52:46.993199","exception":false,"start_time":"2023-10-26T17:52:45.067783","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize some samples from the validation set\nplt.figure(figsize=(10, 40))\ns, e = 10, 14\nindex = 1\n\nfor i, j in zip(X_val[s:e], Y_val[s:e]):\n    plt.subplot(10, 2, index)\n    plt.imshow(i)\n    plt.title('Ground truth image')\n    \n    plt.subplot(10, 2, index+1)\n    plt.imshow(colorize_image(j, color_dict))\n    plt.title('Ground truth mask')\n    index += 2","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:52:47.023421Z","iopub.status.busy":"2023-10-26T17:52:47.022955Z","iopub.status.idle":"2023-10-26T17:52:48.963603Z","shell.execute_reply":"2023-10-26T17:52:48.962225Z"},"papermill":{"duration":1.960942,"end_time":"2023-10-26T17:52:48.96766","exception":false,"start_time":"2023-10-26T17:52:47.006718","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.019903,"end_time":"2023-10-26T17:52:49.009391","exception":false,"start_time":"2023-10-26T17:52:48.989488","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# free the RAM by collecting the garbage\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:52:49.049083Z","iopub.status.busy":"2023-10-26T17:52:49.04866Z","iopub.status.idle":"2023-10-26T17:52:49.328817Z","shell.execute_reply":"2023-10-26T17:52:49.327503Z"},"papermill":{"duration":0.302529,"end_time":"2023-10-26T17:52:49.33096","exception":false,"start_time":"2023-10-26T17:52:49.028431","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling | U-Net","metadata":{"papermill":{"duration":0.019218,"end_time":"2023-10-26T17:52:49.369197","exception":false,"start_time":"2023-10-26T17:52:49.349979","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, Concatenate\nfrom keras.models import Model\n\ndef unet(input_size=(*image_size,3)):\n    inputs = Input(input_size)\n    \n    # Encoder\n    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n    \n    # Decoder\n    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n    \n    up6 = Conv2DTranspose(512, 3, strides=2, activation='relu', padding='same', kernel_initializer='he_normal')(drop5)\n    merge6 = Concatenate(axis=3)([conv4, up6])\n    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n    \n    up7 = Conv2DTranspose(256, 3, strides=2, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n    merge7 = Concatenate(axis=3)([conv3, up7])\n    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n    \n    up8 = Conv2DTranspose(128, 3, strides=2, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n    merge8 = Concatenate(axis=3)([conv2, up8])\n    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n    \n    up9 = Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n    merge9 = Concatenate(axis=3)([conv1, up9])\n    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n    \n    outputs = Conv2D(20, 1, activation='softmax')(conv9)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    return model\n\n# model = unet()\n\n# Compile the model\n\n# model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n# model.summary()","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:52:49.409616Z","iopub.status.busy":"2023-10-26T17:52:49.409125Z","iopub.status.idle":"2023-10-26T17:52:49.428869Z","shell.execute_reply":"2023-10-26T17:52:49.427273Z"},"papermill":{"duration":0.043455,"end_time":"2023-10-26T17:52:49.431767","exception":false,"start_time":"2023-10-26T17:52:49.388312","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = tf.keras.models.load_model('/kaggle/input/road-segmentation-for-adas-bdd100k-cpu/trained_model_32_cpu.h5')\nmodel.compile(optimizer=keras.optimizers.Adam(0.0001), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-10-26T17:52:49.474922Z","iopub.status.busy":"2023-10-26T17:52:49.474552Z","iopub.status.idle":"2023-10-26T17:52:54.445627Z","shell.execute_reply":"2023-10-26T17:52:54.444472Z"},"papermill":{"duration":4.995049,"end_time":"2023-10-26T17:52:54.447988","exception":false,"start_time":"2023-10-26T17:52:49.452939","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train the model\ninitial_epoch = 64\nepochs = 66\nbatch_size = 16\n\nmodel.fit(\n    X_train,\n    Y_train,\n    initial_epoch=initial_epoch,\n    epochs=epochs,\n#     validation_data=(X_val, Y_val),\n    batch_size=batch_size\n)","metadata":{"execution":{"iopub.execute_input":"2023-10-26T17:52:54.499325Z","iopub.status.busy":"2023-10-26T17:52:54.498953Z","iopub.status.idle":"2023-10-27T00:57:20.626018Z","shell.execute_reply":"2023-10-27T00:57:20.618286Z"},"papermill":{"duration":25466.23925,"end_time":"2023-10-27T00:57:20.712102","exception":false,"start_time":"2023-10-26T17:52:54.472852","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y_pred = model.predict(X_val)\nY_pred = np.argmax(Y_pred, axis=-1).reshape(-1, *image_size, 1)","metadata":{"execution":{"iopub.execute_input":"2023-10-27T00:57:20.88814Z","iopub.status.busy":"2023-10-27T00:57:20.887504Z","iopub.status.idle":"2023-10-27T01:06:14.560169Z","shell.execute_reply":"2023-10-27T01:06:14.558719Z"},"papermill":{"duration":533.768857,"end_time":"2023-10-27T01:06:14.563091","exception":false,"start_time":"2023-10-27T00:57:20.794234","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize some results from the validation set.\nplt.figure(figsize=(15, 40))\ns, e = 1, 8\nindex = 1\n\nfor i, j, k in zip(X_val[s:e], Y_pred[s:e], Y_val[s:e]):\n    \n    # write these images into file as well\n    cv2.imwrite(f'./out/img-{index}.jpg', i)\n    cv2.imwrite(f'./out/pred-{index}.png', j)\n    cv2.imwrite(f'./out/ground-{index}.png', k)\n    \n    plt.subplot(10, 3, index)\n    plt.imshow(i)\n    plt.title('Ground truth image')\n    \n    plt.subplot(10, 3, index+1)\n    plt.imshow(colorize_image(k, color_dict))\n    plt.title('Ground truth mask')\n    \n    plt.subplot(10, 3, index+2)\n    plt.imshow(colorize_image(j, color_dict))\n    plt.title('Pred mask')\n    index += 3","metadata":{"execution":{"iopub.execute_input":"2023-10-27T01:06:14.726202Z","iopub.status.busy":"2023-10-27T01:06:14.7255Z","iopub.status.idle":"2023-10-27T01:06:25.040742Z","shell.execute_reply":"2023-10-27T01:06:25.038235Z"},"papermill":{"duration":10.421981,"end_time":"2023-10-27T01:06:25.065171","exception":false,"start_time":"2023-10-27T01:06:14.64319","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.171516,"end_time":"2023-10-27T01:06:25.421796","exception":false,"start_time":"2023-10-27T01:06:25.25028","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tensorflow.keras.metrics import MeanIoU, Accuracy\n\nnum_classes = 20\n\ndef compute_metrics(gt_masks, pred_masks):\n    # Reshape the masks to (batch_size, height, width)\n    gt_masks_reshaped = tf.reshape(gt_masks, [-1, tf.shape(gt_masks)[-1]])\n    pred_masks_reshaped = tf.reshape(pred_masks, [-1, tf.shape(pred_masks)[-1]])\n\n    # Compute accuracy\n    accuracy_metric = Accuracy()\n    accuracy_metric.update_state(gt_masks_reshaped, pred_masks_reshaped)\n    accuracy = accuracy_metric.result().numpy()\n\n    # Compute mean IoU\n    iou_metric = MeanIoU(num_classes=num_classes)\n    iou_metric.update_state(gt_masks_reshaped, pred_masks_reshaped)\n    miou = iou_metric.result().numpy()\n\n    return accuracy, miou\n\n# Example usage\n# Assuming you have ground_truth_masks and predicted_masks as TensorFlow tensors\n\naccuracy, miou = compute_metrics(Y_val, Y_pred)\n\n# Print the metric values\nprint(\"Accuracy:\", accuracy)\nprint(\"Mean IoU:\", miou)","metadata":{"execution":{"iopub.execute_input":"2023-10-27T01:06:25.7734Z","iopub.status.busy":"2023-10-27T01:06:25.771908Z","iopub.status.idle":"2023-10-27T01:06:31.650209Z","shell.execute_reply":"2023-10-27T01:06:31.648541Z"},"papermill":{"duration":6.066845,"end_time":"2023-10-27T01:06:31.654129","exception":false,"start_time":"2023-10-27T01:06:25.587284","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save the model\nmodel.save('./trained_model_33_cpu.h5')","metadata":{"execution":{"iopub.execute_input":"2023-10-27T01:06:31.993158Z","iopub.status.busy":"2023-10-27T01:06:31.992109Z","iopub.status.idle":"2023-10-27T01:06:33.496808Z","shell.execute_reply":"2023-10-27T01:06:33.494169Z"},"papermill":{"duration":1.683633,"end_time":"2023-10-27T01:06:33.501117","exception":false,"start_time":"2023-10-27T01:06:31.817484","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}